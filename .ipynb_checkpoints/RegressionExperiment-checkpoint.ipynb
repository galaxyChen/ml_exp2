{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin to train\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (16281,124) and (1,124) not aligned: 124 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-90744c39f374>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-90744c39f374>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(X, y, val_x, val_y)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[0mnum_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[0mlamb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"default acc: %f\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minit_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-90744c39f374>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(X, y, w)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m     \u001b[0mnum_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[0mnum_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\numpy\\matrixlib\\defmatrix.py\u001b[0m in \u001b[0;36m__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    307\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m             \u001b[1;31m# This promotes 1-D vectors to row vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0masmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__rmul__'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (16281,124) and (1,124) not aligned: 124 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# -*- coding: utf-8 -*-\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    s = 1/(1+np.exp(-z.A1))\n",
    "    s = np.array(list(map(lambda x : x,s)))\n",
    "    return s\n",
    "\n",
    "def loss(X,y,w,lamb):\n",
    "    h = sigmoid(X*w)\n",
    "    m = y.shape[0]\n",
    "    y_temp = np.array(list(map(lambda x:1 if x[0]==1 else 0,y.A)))\n",
    "    J = -y_temp*np.log(h)-(1-y_temp)*np.log(1-h)\n",
    "    w_reg = w\n",
    "    w_reg[0]=0\n",
    "    return J.sum()/m+lamb/(2*m)*((w_reg.A**2).sum())\n",
    "\n",
    "def gradient(X,y,w,lamb):\n",
    "    m = y.shape[0]\n",
    "    y_temp = np.array(list(map(lambda x:1 if x[0]==1 else 0,y.A)))\n",
    "    h_y = np.matrix(sigmoid(X*w)-y_temp)\n",
    "    dJ = X.T*(h_y.T)/m\n",
    "    w_reg = w\n",
    "    w_reg[0] = 0\n",
    "    g = dJ+w_reg*lamb/m\n",
    "    return g\n",
    "    \n",
    "def gradientDecent(X,y,w,alpha,lamb,num_rounds,val_x,val_y):\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    print(\"origin train loss:%f\"%loss(X,y,w,lamb))\n",
    "    train_loss_history.append(loss(X,y,w,lamb))\n",
    "    print(\"origin validation loss:%f\"%loss(val_x,val_y,w,lamb))\n",
    "    val_loss_history.append(loss(val_x,val_y,w,lamb))\n",
    "    \n",
    "    for i in range(num_rounds):\n",
    "        w = w - gradient(X,y,w,lamb)*alpha\n",
    "        train_loss_history.append(loss(X,y,w,lamb))\n",
    "        val_loss_history.append(loss(val_x,val_y,w,lamb))\n",
    "        \n",
    "    return w,train_loss_history,val_loss_history\n",
    "\n",
    "\n",
    "def NAG(X,y,w,alpha,lamb,num_rounds,val_x,val_y):\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    train_loss_history.append(loss(X,y,w,lamb))\n",
    "    val_loss_history.append(loss(val_x,val_y,w,lamb))\n",
    "    print(\"NAG begin\")\n",
    "    \n",
    "    r = 0.9\n",
    "    v = np.zeros(w.shape)\n",
    "    \n",
    "    for i in range(num_rounds):\n",
    "        random = list(set(np.random.randint(0,X.shape[0],size=100)))\n",
    "        gdx = X[random]\n",
    "        gdy = y[random]\n",
    "        \n",
    "        _w = w - r*v\n",
    "        gt = gradient(gdx,gdy,_w,lamb)\n",
    "        v = r*v + alpha*gt\n",
    "        w = w - v\n",
    "        train_loss_history.append(loss(X,y,w,lamb))\n",
    "        val_loss_history.append(loss(val_x,val_y,w,lamb))\n",
    "        \n",
    "    return w,train_loss_history,val_loss_history\n",
    "\n",
    "def RMSProp(X,y,w,alpha,lamb,num_rounds,val_x,val_y):\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    train_loss_history.append(loss(X,y,w,lamb))\n",
    "    val_loss_history.append(loss(val_x,val_y,w,lamb))\n",
    "    print(\"RMSProp begin\")\n",
    "    \n",
    "    r = 0.9\n",
    "    v = np.zeros(w.shape)\n",
    "    e = 1e-8\n",
    "    \n",
    "    for i in range(num_rounds):\n",
    "        random = list(set(np.random.randint(0,X.shape[0],size=100)))\n",
    "        gdx = X[random]\n",
    "        gdy = y[random]\n",
    "        \n",
    "        gt = gradient(gdx,gdy,w,lamb)\n",
    "        gt_2 = np.matrix((gt.A)**2)\n",
    "        v = r*v + (1-r)*gt_2\n",
    "        w = w - 0.001/np.sqrt(v+e).A*(gt.A)\n",
    "        train_loss_history.append(loss(X,y,w,lamb))\n",
    "        val_loss_history.append(loss(val_x,val_y,w,lamb))\n",
    "        \n",
    "    return w,train_loss_history,val_loss_history\n",
    "\n",
    "def AdaDelta(X,y,w,alpha,lamb,num_rounds,val_x,val_y):\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    train_loss_history.append(loss(X,y,w,lamb))\n",
    "    val_loss_history.append(loss(val_x,val_y,w,lamb))\n",
    "    print(\"AdaDelta begin\")\n",
    "    \n",
    "    r = 0.95\n",
    "    v = np.zeros(w.shape)\n",
    "    e = 1e-6\n",
    "    t = np.matrix(np.zeros(w.shape))\n",
    "    \n",
    "    for i in range(num_rounds):\n",
    "        random = list(set(np.random.randint(0,X.shape[0],size=100)))\n",
    "        gdx = X[random]\n",
    "        gdy = y[random]\n",
    "        \n",
    "        gt = gradient(gdx,gdy,w,lamb)\n",
    "        gt_2 = np.matrix((gt.A)**2)\n",
    "        v = r*v + (1-r)*gt_2\n",
    "        dw = -(np.sqrt(t+e).A/np.sqrt(v+e).A)*gt.A\n",
    "        w = w + dw\n",
    "        t = r*t+(1-r)*(t.A*t.A)\n",
    "        train_loss_history.append(loss(X,y,w,lamb))\n",
    "        val_loss_history.append(loss(val_x,val_y,w,lamb))\n",
    "        \n",
    "    return w,train_loss_history,val_loss_history\n",
    "\n",
    "def Adam(X,y,w,alpha,lamb,num_rounds,val_x,val_y):\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    train_loss_history.append(loss(X,y,w,lamb))\n",
    "    val_loss_history.append(loss(val_x,val_y,w,lamb))\n",
    "    print(\"Adam begin\")\n",
    "    \n",
    "    r = 0.999\n",
    "    v = np.zeros(w.shape)\n",
    "    e = 1e-8\n",
    "    m = np.zeros(w.shape)\n",
    "    b1 = 0.9\n",
    "    \n",
    "    \n",
    "    for i in range(num_rounds):\n",
    "        random = list(set(np.random.randint(0,X.shape[0],size=100)))\n",
    "        gdx = X[random]\n",
    "        gdy = y[random]\n",
    "        \n",
    "        gt = gradient(gdx,gdy,w,lamb)\n",
    "        gt_2 = np.matrix((gt.A)**2)\n",
    "        v = r*v + (1-r)*gt_2\n",
    "        m = b1*m + (1-b1)*gt.A\n",
    "        alp = 0.001*np.sqrt(1-r)/(1-b1)\n",
    "        w = w - alp*m/np.sqrt(v+e).A\n",
    "        train_loss_history.append(loss(X,y,w,lamb))\n",
    "        val_loss_history.append(loss(val_x,val_y,w,lamb))\n",
    "        \n",
    "    return w,train_loss_history,val_loss_history\n",
    "\n",
    "\n",
    "\n",
    "def train(X,y,val_x,val_y):\n",
    "    m = X.shape[1]\n",
    "    init_w = np.matrix(np.zeros((m,1)))\n",
    "    print(\"begin to train\")\n",
    "    alpha=0.1\n",
    "    num_rounds=500\n",
    "    lamb = 1\n",
    "    print(\"default acc: %f\"%predict(val_x,val_y,init_w))\n",
    "    print(\"\")\n",
    "    \n",
    "    w,train_loss_history,NAG_loss_history = gradientDecent(X,y,init_w,alpha,lamb,num_rounds,val_x,val_y)\n",
    "    print(\"Gradient Decent acc: %f\"%predict(val_x,val_y,w))\n",
    "    print(\"\")\n",
    "    \n",
    "    w,train_loss_history,NAG_loss_history = NAG(X,y,init_w,alpha,lamb,num_rounds,val_x,val_y)\n",
    "    print(\"NAG acc: %f\"%predict(val_x,val_y,w))\n",
    "    print(\"\")\n",
    "    w,train_loss_history,RMSProp_loss_history = RMSProp(X,y,init_w,alpha,lamb,num_rounds,val_x,val_y)\n",
    "    print(\"RMSProp acc: %f\"%predict(val_x,val_y,w))\n",
    "    print(\"\")\n",
    "    w,train_loss_history,AdaDelta_loss_history = AdaDelta(X,y,init_w,alpha,lamb,num_rounds,val_x,val_y)\n",
    "    print(\"AdaDelta acc: %f\"%predict(val_x,val_y,w))\n",
    "    print(\"\")\n",
    "    w,train_loss_history,Adam_loss_history = Adam(X,y,init_w,alpha,lamb,num_rounds,val_x,val_y)\n",
    "    print(\"Adam acc: %f\"%predict(val_x,val_y,w))\n",
    "    print(\"\")\n",
    "    plt.plot(np.arange(num_rounds+1),NAG_loss_history,label='NAG loss')\n",
    "    plt.plot(np.arange(num_rounds+1),RMSProp_loss_history,label='RMSProp loss')\n",
    "    plt.plot(np.arange(num_rounds+1),AdaDelta_loss_history,label='AdaDelta loss')\n",
    "    plt.plot(np.arange(num_rounds+1),Adam_loss_history,label='Adam loss')\n",
    "    plt.legend(loc=1)\n",
    "    plt.xlabel('number_of_rounds')\n",
    "    plt.ylabel('loss')\n",
    "    return w\n",
    "    \n",
    "def predict(X,y,w):\n",
    "    pred = sigmoid(X*w)\n",
    "    num_p = (y==1).sum()\n",
    "    num_n = (y==-1).sum()\n",
    "    pred = pred/(1-pred)\n",
    "    pred_y = list(map(lambda x:1 if x>num_p/num_n else -1,pred))\n",
    "    acc = (y.A1==pred_y).sum()/len(y.A)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def getData():\n",
    "    X,y = datasets.load_svmlight_file('./a9a',n_features=123)\n",
    "    X = np.matrix(X.toarray())\n",
    "    ones = np.matrix(np.ones((X.shape[0],1)))\n",
    "    train_x = np.concatenate((ones,X),axis=1)\n",
    "    train_y = np.matrix(y).T\n",
    "    \n",
    "    X,y = datasets.load_svmlight_file('./a9a.t',n_features=123)\n",
    "    X = np.matrix(X.toarray())\n",
    "    ones = np.matrix(np.ones((X.shape[0],1)))\n",
    "    test_x = np.concatenate((ones,X),axis=1)\n",
    "    test_y = np.matrix(y).T\n",
    "    return train_x,test_x,train_y,test_y\n",
    "    \n",
    "\n",
    "train_x,test_x,train_y,test_y = getData()\n",
    "w = train(train_x,train_y,test_x,test_y)\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the Logistic Regression,which is actually a classification model rather than regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
